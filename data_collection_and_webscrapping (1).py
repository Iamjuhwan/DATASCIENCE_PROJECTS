# -*- coding: utf-8 -*-
"""Data Collection and WebScrapping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DM9G1AyLMdzog_SEYisoPmAD__lhx6bo
"""

from google.colab import files
files.upload()

!rm -r ~/.kaggle
!mkdir ~/.kaggle
!mv ./kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets list

!kaggle datasets download andrewgeorgeissac/hotel-price-data-of-cities-in-india-makemytrip

import zipfile
zip_ref = zipfile.ZipFile('/content/hotel-price-data-of-cities-in-india-makemytrip.zip', 'r')
zip_ref.extractall('/content')
zip_ref.close()

import pandas as pd

df = pd.read_csv('/content/bangalore.csv')
df.head()





"""## Web Scraping

### Step 1: Sending HTTP Requests
"""

pip install requests beautifulsoup4

import requests

# Send an HTTP GET request
url = "https://cnn.com"
response = requests.get(url)

# Check the response status code
print("Response Status Code:", response.status_code)

# Print the first 500 characters of the response content
print("Response Content:")
print(response.text[:500])

print(response.content)

"""## Tep 2:  Parsing HTML using BeautifulSoup"""

from bs4 import BeautifulSoup

# Parse the HTML content
soup = BeautifulSoup(response.content, "html.parser")

# Print the prettified HTML
print(soup.prettify())



"""### Step 3: Extracting the data"""

# Find the first <h1> tag and extract its text
header = soup.find("h1")
print("Header:", header)

# Find all <a> tags and extract their href attributes
links = soup.find_all("a")
for link in links:
    print("Link:", link.get("href"))



"""### Task 4: Handling Pagination"""

# Example of navigating through pagination
base_url = "https://cnn.com/page={}"

for page_num in range(1, 6):
    page_url = base_url.format(page_num)
    page_response = requests.get(page_url)
    # Parse and extract data from the page_response

"""### Step 5: Getting Pictures from a pacticular website, using beautiful soup"""

import os

url = "https://www.propertypro.ng/"

response = requests.get(url)

if response.status_code == 200:
    soup = BeautifulSoup(response.content, 'html.parser')

    # Scrape images
    image_folder = 'images'
    os.makedirs(image_folder, exist_ok=True)
    image_tags = soup.find_all('img')

    for img_tag in image_tags:
        img_url = img_tag['src']
        if not img_url.startswith('http'):
            img_url = url + img_url
        img_response = requests.get(img_url)

        # Save the image to the images folder
        with open(os.path.join(image_folder, os.path.basename(img_url)), 'wb') as img_file:
            img_file.write(img_response.content)

    # Scrape text
    text_data = []
    text_tags = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'span', 'div'])

    for tag in text_tags:
        text_data.append(tag.get_text())

    # Print the text data
    print("\n".join(text_data))

else:
    print(f"Failed to fetch data from {url}. Status code: {response.status_code}")



"""### Getting data from RESTful API"""

import requests #import resquest to get data from url
import json #json to use json files


api_key =''

city_name = "Lagos"

api_url = f"https://api.openweathermap.org/data/3.0/onecall/day_summary?lat=39.099724&lon=-94.578331&dt=1643803200&appid={api_key}"


response = requests.get(api_url).json()

print(response)

import requests


# Set your Google Custom Search API key and search engine ID
api_key = "AIzaSyDAIu0xc1ad6jSR4WNQfvyDjqiYVbpHf60"
search_engine_id = "07456a506c9d045c6"

# Set the search query
query = "images of dogs"

# Construct the API request URL
url = f"https://www.googleapis.com/customsearch/v1?q={query}&cx={search_engine_id}&key={api_key}&searchType=image"

# Send the API request
response = requests.get(url)

# Parse the JSON response
data = response.json()

# Extract and print image URLs
if "items" in data:
    for item in data["items"]:
        print("Image URL:", item["link"])
else:
    print("No images found.")

search_id = ''

import requests
import os

# Set your Google Custom Search API key and search engine ID
api_key = "AIzaSyDAIu0xc1ad6jSR4WNQfvyDjqiYVbpHf60"
search_engine_id = "07456a506c9d045c6"

# Set the search query
query = "cat_images"

# Construct the API request URL
url = f"https://www.googleapis.com/customsearch/v1?q={query}&cx={search_engine_id}&key={api_key}&searchType=image"

# Send the API request
response = requests.get(url)

# Parse the JSON response
data = response.json()

# Create a directory to store downloaded images
output_directory = "downloaded_images"
os.makedirs(output_directory, exist_ok=True)

# Download and save images
if "items" in data:
    for index, item in enumerate(data["items"]):
        image_url = item["link"]
        image_response = requests.get(image_url)
        image_extension = image_url.split(".")[-1]
        image_filename = os.path.join(output_directory, f"image_{index + 1}.{image_extension}")

        with open(image_filename, "wb") as image_file:
            image_file.write(image_response.content)
            print(f"Downloaded {image_filename}")
else:
    print("No images found.")



"""### Dealing with Dynamic Content
selenium is use to automate web drowser informations
"""

pip install selenium

from selenium import webdriver


from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

chrome_options = Options()
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--headless')
chrome_options.add_argument('--disable-gpu')
chrome_options.add_argument('--disable-dev-shm-usage')
chrome_options.add_argument('--profile-directory=Default')
chrome_options.add_argument('--user-data-dir=~/.config/google-chrome')

driver = webdriver.Chrome(options=chrome_options)
url = 'https://africa.espn.com/'

driver.get(url)



pip install webdriver-manager

from selenium.webdriver.common.by import By

# Find reviews using Selenium
review_elements = driver.find_elements(By.XPATH, "//div[@data-review-id]")

# Extract and print review text
for element in review_elements:
    review_text = element.text
    print("Review:", review_text)

# Close the WebDriver
driver.quit()

from selenium.webdriver.common.by import By
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

chrome_options = Options()
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--headless')
chrome_options.add_argument('--disable-gpu')
chrome_options.add_argument('--disable-dev-shm-usage')
chrome_options.add_argument('--profile-directory=Default')
# chrome_options.add_argument('--user-data-dir=~/.config/google-chrome') # Removed this line

driver = webdriver.Chrome(options=chrome_options)
url = 'https://africa.espn.com/'
driver.get(url)

# Print the page source for inspection
print(driver.page_source)

# Wait for the dynamic content element to be present
try:
    element = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.ID, "dynamic-content"))
    )
    dynamic_content = element.text
    print("Dynamic Content:", dynamic_content)
except:
    print("Element with ID 'dynamic-content' not found within the given time.")


# Remember to close the WebDriver
driver.quit()



"""### Regular Expressions

The basics of using regular expressions (regex) in Python. and it covers finding and matching single and multiple strings, grouping, and substitution using the re module.


---


Regular expressions (regex) are powerful tools for pattern matching and manipulation of text data. They allow to define complex patterns and search for or manipulate strings based on those patterns.

Finding and Matching Strings



"""

import re
text = "This course will introduce the basics of data science"
match = re.search(r"data science", text)
print(match.start())

"""### Finding Multiple Strings"""

# Search for the word "apple" in a text
text = "I like apples and bananas."
pattern = r"apple"

match = re.search(pattern, text)
if match:
    print("Match found:", match.group())
else:
    print("No match found.")

print(match.start())

pattern = r"[aeiou]"
text = "The quick brown fox jumps over the lazy dog."

matches = re.findall(pattern, text)
print("Matches:", matches)

#Skipping Characters using the caret sign groups
pattern = r"[^aeiou]"
text = "The quick brown fox jumps over the lazy dog."

matches = re.findall(pattern, text)
print("Matches:", matches)

#Skipping Characters using the caret sign groups
pattern = r"[^aeiou]"
text = "The quick brown fox jumps over the lazy dog."

matches = re.findall(pattern, text)
print("Matches:", matches)

# Find all occurrences of numbers in a text
text = "The numbers 1, 23, and 456 are present."
pattern = r"\d+"

matches = re.findall(pattern, text)
print("Matches:", matches)

#for Alphanumeric text

pattern = r"\w"
text = "The_quick_brown_fox_2023."

matches = re.findall(pattern, text)
print("Matches:", matches)

# matching whitespace characters

pattern = r"\s"
text = "This is a sentence with spaces."

matches = re.findall(pattern, text)
print("Matches:", matches)

