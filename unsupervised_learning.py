# -*- coding: utf-8 -*-
"""Unsupervised Learning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fB4N4-YFXPeUIPCYs3Y1ui4jRGOR3jbM

# Unsupervised Learning

Dimentionalty Reduction

PCA

t-SNE

Clustering

k-means

GMM

Implementing k-means
"""

import pandas as pd
import random
import numpy as np
import matplotlib.pyplot as plt

from pandas.plotting import scatter_matrix
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("/content/drive/MyDrive/data.csv")
df

df = df.sample(frac=1).reset_index(drop=True)
df

df_target = df['target']
df = df.drop('target', axis=1)

df

scatter_matrix(df, alpha=0.6, figsize=(8, 8), diagonal='hist')

featue1 = "X3"
featue2 = "X4"

ax = df.plot(x=featue1, y=featue2, kind='scatter', c=df_target, cmap='viridis') #
for i in range(3):
    ax.text(df.loc[i, featue1], df.loc[i, featue2], 'Point{}'.format(i), weight='bold')
plt.show()

featue3 = "X1"
featue4 = "X2"

ax = df.plot(x=featue3, y=featue4, kind='scatter', c=df_target, cmap='viridis') #
for i in range(3):
    ax.text(df.loc[i, featue1], df.loc[i, featue2], 'Point{}'.format(i), weight='bold')
plt.show()

# create a PCA object with 2 components
pca = PCA(n_components=2)

# fit and transform the data
X_pca = pca.fit_transform(df)

# plot the PCA components
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df_target, cmap='viridis')
plt.xlabel('PC1')
plt.ylabel('PC2')

# add text labels to the first 3 points
for i in range(3):
    plt.text(X_pca[i, 0], X_pca[i, 1], 'Point{}'.format(i), weight='bold')

plt.show()



"""## t-SNE"""

tsne = TSNE(n_components=2,perplexity=20, random_state=42) # default value is 30

X_embedded = tsne.fit_transform(df)

# plot the embedding
plt.scatter(X_embedded[:,0], X_embedded[:,1], c=df_target, cmap='viridis')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')

# add text labels to the first 3 points
for i in range(3):
    plt.text(X_embedded[i, 0], X_embedded[i, 1], 'Point{}'.format(i), weight='bold')

plt.show()

from sklearn import datasets

noisy_circles = datasets.make_circles(
    n_samples=1500, factor=0.5, noise=0.05, random_state=170
)

plt.scatter(noisy_circles[0][:,0], noisy_circles[0][:,1],c=noisy_circles[1]) #
plt.show()

# creating a PCA object with 2 components
pca = PCA(n_components=1)

# fit and transform the data
X_pca_circle = pca.fit_transform(noisy_circles[0])

# plot the PCA components
plt.scatter(X_pca_circle[:, 0], np.random.uniform(0, 1, size=len(X_pca_circle)), c=noisy_circles[1]) #
plt.xlabel('PC1')
plt.ylabel('Noise')

plt.show()

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=3, n_init='auto')

# fit the data
kmeans.fit(df)

# get the cluster labels
labels = kmeans.labels_

# plot the clusters
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels)
plt.xlabel('PC1')
plt.ylabel('PC2')

plt.show()

tsne = TSNE(n_components=1, perplexity=40, n_iter=15000, learning_rate=1000, random_state=42)
X_tsne = tsne.fit_transform(noisy_circles[0])

# plot the embedding
plt.scatter(X_tsne[:,0], np.random.uniform(0, 1, size=len(X_tsne)), c=noisy_circles[1])
plt.xlabel('t-SNE Component 1')
plt.ylabel('Noise')



"""### MNIST Dataset"""

ds = pd.read_csv("/content/drive/MyDrive/mnist_train.csv")
ds

X = ds.iloc[:, 1:][:3000]
y = ds.iloc[:, 0][:3000]

# create a PCA object with 2 components
pca = PCA(n_components=2)

# fit and transform the data
X_pca = pca.fit_transform(X)

# print the explained variance ratio
print(pca.explained_variance_ratio_)

# plot the PCA components
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.show()

tsne = TSNE(n_components=2, random_state=42, perplexity=15)
X_embedded = tsne.fit_transform(X)

plt.scatter(X_embedded[:,0], X_embedded[:,1], c=y)
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')

# add text labels to the first 3 points


plt.show()



"""### Clustering"""

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=3, n_init='auto')

# fit the data
kmeans.fit(ds)

# get the cluster labels
labels = kmeans.labels_

# plot the clusters
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels)
plt.xlabel('PC1')
plt.ylabel('PC2')

plt.show()





inertias = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, n_init='auto')
    kmeans.fit(df)
    inertias.append(kmeans.inertia_)

plt.plot(range(1, 11), inertias)
plt.title('Elbow Plot')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.show()



from sklearn.mixture import GaussianMixture

# create a GaussianMixture object with 3 components
gmm = GaussianMixture(n_components=3)

# fit the data
gmm.fit(X) # Fit GMM on X, which has 3000 rows

# get the cluster labels
labels = gmm.predict(X) # Predict labels for X

# plot the clusters
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels) # Now labels will have 3000 elements
plt.xlabel('PC1')
plt.ylabel('PC2')


plt.show()



# create a list to store the log-likelihood scores for different number of clusters
log_likelihood_scores = []

# fit the Gaussian Mixture model for different number of clusters
for k in range(1, 11):
    gmm = GaussianMixture(n_components=k)
    gmm.fit(df)
    log_likelihood_scores.append(gmm.score(df)) #try gm.bic(df)

# plot the elbow plot
plt.plot(range(1, 11), log_likelihood_scores)
plt.title('Elbow Plot for Gaussian Mixture Model')
plt.xlabel('Number of Clusters')
plt.ylabel('Log-Likelihood Score')
plt.show()



"""### k-means Implementation"""

def k_means(X, k, max_iterations=np.inf):
    centroids = X[np.random.choice(X.shape[0], k, replace=False), :]
    labels = None
    old_labels = None
    iteration = 0
    while (old_labels is None or not np.array_equal(labels,old_labels)) and iteration < max_iterations:
      old_labels = labels
      iteration += 1
      distance = np.sqrt(((X-centroids[:,np.newaxis])**2).sum(axis=2))
      labels = np.argmin(distance, axis=0)
      if iteration < max_iterations:
        for j in range(k):
          centroids[j] = X[labels == j].mean(axis=0)

    return labels, centroids

from sklearn.datasets import make_blobs

# create data with 3 round clusters
X, y = make_blobs(n_samples=300, centers=3, cluster_std=0.5, random_state=0)

# plot the data
import matplotlib.pyplot as plt
plt.scatter(X[:, 0], X[:, 1], c=y)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()



np.random.seed(837)
labels, centroids = k_means(X, k=3, max_iterations=100)

plt.scatter(X[:, 0], X[:, 1], c=labels)
plt.scatter(centroids[:, 0], centroids[:, 1], marker='*', s=300, c='r')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

X, y = datasets.make_blobs(n_samples=1500, random_state=170)
transformation = [[0.6, -0.6], [-0.4, 0.8]]
X = np.dot(X, transformation)

# plot the data
plt.scatter(X[:, 0], X[:, 1], c=y)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

np.random.seed(837)
labels, centroids = k_means(X, k=3, max_iterations=100)

plt.scatter(X[:, 0], X[:, 1], c=labels)
plt.scatter(centroids[:, 0], centroids[:, 1], marker='*', s=300, c='r')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()



from sklearn.metrics import silhouette_score

# Assuming 'labels' contains the cluster labels from the k-means clustering on X_pca
# Make sure to use the labels generated from fitting k-means on X_pca
# If you ran k-means on a different dataset last, you might need to re-run it on X_pca first.

# Let's re-run k-means on X_pca to ensure we have the correct labels
kmeans_pca = KMeans(n_clusters=3, n_init='auto', random_state=42)
kmeans_pca.fit(X_pca)
labels_pca = kmeans_pca.labels_

# Calculate the silhouette score
silhouette_avg = silhouette_score(X_pca, labels_pca)

print(f"The Silhouette Score for the k-means clustering on PCA-reduced data is: {silhouette_avg}")



# Perform PCA specifically on the Iris dataset (df)
pca_iris = PCA(n_components=2)
X_pca_iris = pca_iris.fit_transform(df)

# Now X_pca_iris contains the PCA-reduced Iris data (150 rows)

